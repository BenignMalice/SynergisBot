# Optimized 10-Second Interval Implementation Plan

**Date:** 2025-12-21  
**Version:** 1.7 (Final Review Round 4 - All Issues Fixed)  
**Status:** ðŸ“‹ **PLANNING**  
**Target:** M1 Micro-Scalp Plans (XAUUSDc, BTCUSDc)

---

## âš ï¸ **CRITICAL INTEGRATION FIXES APPLIED**

**Review Date:** 2025-12-21 (Updated: Final Review - Round 4)  
**Issues Found:** 25 critical integration/logic/implementation issues  
**Status:** âœ… **ALL FIXED**

### **Fixed Issues (24 Total):**

**Integration Issues (10):**
1. âœ… **Config Loading:** Changed from separate JSON loader to integration with existing `self.config` dict structure
2. âœ… **Cache Structure:** Fixed to use existing `_m1_data_cache` and `_m1_cache_timestamps` (not non-existent `_cache` and `_cache_expiry`)
3. âœ… **Cache Timestamp Format:** Fixed to use float timestamps (time.time()) not datetime objects for `_m1_cache_timestamps`
4. âœ… **Candle Time Tracking:** Added separate `_m1_latest_candle_times` dict for candle-close detection
5. âœ… **TradePlan Modification:** Changed to use separate tracking dicts instead of modifying dataclass
6. âœ… **Monitor Loop:** Fixed to preserve ALL existing logic (expiration, re-evaluation, M1 refresh, weekend checks, etc.)
7. âœ… **Price Fetching:** Added proper error handling and fallback to `mt5.symbol_info_tick()` if `get_quote()` fails
8. âœ… **Conditional Checks Config:** Fixed to use `optimized_intervals.conditional_checks` path
9. âœ… **Database Operations:** Noted that existing `_update_plan_status()` already uses write queue efficiently
10. âœ… **Thread Management:** Added proper start/stop lifecycle for pre-fetch thread

**Logic & Implementation Issues (15):**
11. âœ… **Check Order Logic:** Fixed adaptive interval check to happen AFTER expiration/cancellation checks (not before)
12. âœ… **Last Check Time Update:** Fixed to only update `_plan_last_check` if plan was actually checked (not skipped)
13. âœ… **Error Handling:** Added error handling for adaptive interval calculation and price fetching failures
14. âœ… **Performance Optimization:** Only fetch prices if there are pending plans and features enabled
15. âœ… **Thread Safety Documentation:** Documented that tracking dicts are accessed only from monitor thread (no locks needed)
16. âœ… **Dict Initialization:** Fixed `_m1_latest_candle_times` to be initialized in `__init__` (not with hasattr check)
17. âœ… **Symbol Normalization:** Added symbol normalization in `_invalidate_cache_on_candle_close()` for consistency
18. âœ… **Candle Time Format:** Added proper handling for all candle time formats (datetime, int, float) with UTC timezone
19. âœ… **Pre-fetch Error Handling:** Added error handling for pre-fetch failures (non-critical, continue with other symbols)
20. âœ… **Prefetch Thread Initialization:** Added `prefetch_thread = None` initialization in `__init__` for proper cleanup
21. âœ… **Config Deep Merge:** Fixed config merging to use deep merge for nested dicts (preserves existing config)
22. âœ… **Order of Operations:** Fixed to get plans BEFORE fetching prices (prevents using undefined variable)
23. âœ… **Quote Error Handling:** Added proper error handling for Quote dataclass attribute access
24. âœ… **Plan Removal Check:** Fixed to set `plan_was_executed = True` atomically during plan removal (not after separate check)
25. âœ… **Plan Removal Timing:** Fixed to set `plan_was_executed` flag during removal operation (atomic with lock), not via separate existence check

### **Key Integration Principles:**

- âœ… **Feature Flag:** All optimizations disabled by default (safe rollout)
- âœ… **Graceful Degradation:** System works normally if config disabled
- âœ… **Backward Compatible:** Existing functionality fully preserved
- âœ… **Thread Safety:** Uses existing `plans_lock` where needed
- âœ… **Check Order:** Adaptive checks happen AFTER expiration/cancellation but BEFORE expensive condition checks
- âœ… **Error Handling:** Graceful degradation if price fetch fails or adaptive calculation errors
- âœ… **Performance:** Optimized to only fetch prices when needed

---

## ðŸ“Š **Executive Summary**

**Goal:** Implement optimized 10-second check interval for M1 micro-scalp plans with smart caching, conditional checks, and adaptive intervals to catch more fast-moving setups while maintaining efficient resource usage.

**Expected Benefits:**
- âœ… Catch 50-70% more fast-moving M1 micro-scalp opportunities
- âœ… Reduce missed trades by 50-70%
- âœ… Maintain CPU usage at 8-18% (optimized from 12-30%)
- âœ… Reduce cache misses by 30-50%
- âœ… Reduce unnecessary checks by 40-60%

**Resource Impact:**
- CPU: 8-18% (optimized)
- RAM: +10-20 MB (optimized)
- SSD: +4-8 KB/min (optimized)
- MT5 API: 20-40 calls/min (optimized)

---

## ðŸŽ¯ **Implementation Components**

### **1. Adaptive Check Intervals**
- **M1 micro-scalp plans:** 10 seconds (when price is close)
- **M1 micro-scalp plans:** 30 seconds (when price is far)
- **M5+ plans:** 30 seconds (unchanged)
- **Dynamic adjustment:** Based on price proximity to entry

### **2. Smart Caching**
- **Cache TTL:** 15-30 seconds (shorter for 10s checks)
- **Cache invalidation:** On M1 candle close (not time-based)
- **Pre-fetching:** 2-3 seconds before cache expiry
- **Cache hit rate target:** 70-80% (up from 50-60%)

### **3. Conditional Checks**
- **Price proximity filter:** Skip checks when price is >2Ã— tolerance away
- **Entry zone detection:** Only check when price is within 2Ã— tolerance
- **Reduction target:** 40-60% fewer unnecessary checks

### **4. Batch Operations**
- **MT5 API batching:** Group all calls per cycle
- **Database batching:** Batch reads and writes
- **Parallel processing:** Where possible

---

## ðŸ“‹ **Implementation Steps**

### **Phase 1: Configuration & Infrastructure (Day 1)**

#### **Step 1.1: Integrate Configuration with Existing Config Structure**

**File:** `auto_execution_system.py` (modify `__init__` method)

**Changes:**
1. Add optimized intervals config to existing `self.config` dict structure
2. Load from JSON file if exists, otherwise use defaults
3. Integrate with existing config loading pattern

**Code Location:** In `__init__` method, after existing config initialization (around line 717)

```python
# Add optimized intervals config to existing config structure
optimized_config_path = Path("config/auto_execution_optimized_intervals.json")
if optimized_config_path.exists():
    try:
        with open(optimized_config_path, 'r') as f:
            optimized_config = json.load(f)
            # Merge into existing config (deep merge for nested dicts)
            # If JSON has 'optimized_intervals' key, it will replace/merge that section
            if 'optimized_intervals' in optimized_config:
                # Deep merge nested dicts to preserve existing config values
                if 'optimized_intervals' in self.config:
                    # Merge nested dicts recursively
                    def _deep_merge(base: dict, override: dict) -> dict:
                        """Recursively merge nested dicts"""
                        # Create a copy to avoid modifying original
                        result = dict(base)
                        for key, value in override.items():
                            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                                result[key] = _deep_merge(result[key], value)
                            else:
                                result[key] = value
                        return result
                    self.config['optimized_intervals'] = _deep_merge(
                        self.config.get('optimized_intervals', {}),
                        optimized_config['optimized_intervals']
                    )
                else:
                    self.config['optimized_intervals'] = optimized_config['optimized_intervals']
            else:
                # If JSON doesn't have 'optimized_intervals', merge top-level keys
                self.config.update(optimized_config)
    except Exception as e:
        logger.warning(f"Error loading optimized intervals config: {e}, using defaults")
        # Use defaults
        self.config['optimized_intervals'] = {
            'adaptive_intervals': {'enabled': False},
            'smart_caching': {'enabled': False},
            'conditional_checks': {'enabled': False},
            'batch_operations': {'enabled': False}
        }
else:
    # Use defaults (feature disabled by default)
    self.config['optimized_intervals'] = {
        'adaptive_intervals': {
            'enabled': False,  # Disabled by default - enable via config file
            'default_interval_seconds': 30,
            'plan_type_intervals': {
                'm1_micro_scalp': {
                    'base_interval_seconds': 10,
                    'far_interval_seconds': 30,
                    'close_interval_seconds': 5,
                    'price_proximity_multiplier': 2.0
                },
                'm5_range_scalp': {
                    'base_interval_seconds': 30,
                    'far_interval_seconds': 60,
                    'close_interval_seconds': 20
                },
                'm15_range_scalp': {
                    'base_interval_seconds': 30,
                    'far_interval_seconds': 60,
                    'close_interval_seconds': 20
                }
            }
        },
        'smart_caching': {
            'enabled': False,
            'm1_cache_ttl_seconds': 20,
            'invalidate_on_candle_close': True,
            'prefetch_seconds_before_expiry': 3
        },
        'conditional_checks': {
            'enabled': False,
            'price_proximity_filter': True,
            'proximity_multiplier': 2.0,
            'min_check_interval_seconds': 10
        },
        'batch_operations': {
            'enabled': False,
            'mt5_batch_size': 5,
            'db_batch_size': 10
        }
    }
```

**Optional Config File:** `config/auto_execution_optimized_intervals.json` (create only if custom values needed)

```json
{
  "optimized_intervals": {
    "adaptive_intervals": {
      "enabled": true,
      "default_interval_seconds": 30,
      "plan_type_intervals": {
        "m1_micro_scalp": {
          "base_interval_seconds": 10,
          "far_interval_seconds": 30,
          "close_interval_seconds": 5,
          "price_proximity_multiplier": 2.0
        }
      }
    },
    "smart_caching": {
      "enabled": true,
      "m1_cache_ttl_seconds": 20,
      "invalidate_on_candle_close": true,
      "prefetch_seconds_before_expiry": 3
    },
    "conditional_checks": {
      "enabled": true,
      "price_proximity_filter": true,
      "proximity_multiplier": 2.0,
      "min_check_interval_seconds": 10
    },
    "batch_operations": {
      "enabled": true,
      "mt5_batch_size": 5,
      "db_batch_size": 10
    }
  }
}
```

**Action Items:**
- [x] Integrate config loading into existing `__init__` method
- [x] **CRITICAL:** Use deep merge for nested config dicts (preserve existing config values)
- [x] Create optional config file template
- [x] Document configuration options
- [x] Add feature flag (disabled by default for safety)
- [x] **NOTE:** JSON file should have `{"optimized_intervals": {...}}` structure
- [x] **CREATED:** Config file `config/auto_execution_optimized_intervals.json` with all features enabled

---

#### **Step 1.2: Add Plan Type Detection**

**File:** `auto_execution_system.py`

**Changes:**
1. Add method to detect plan type from conditions/timeframe
2. Store plan type in plan metadata
3. Use plan type for interval selection

**Code Location:** New method in `AutoExecutionSystem` class (after `_load_plans()`)

**IMPORTANT:** TradePlan is a dataclass - do NOT modify it directly. Use a separate tracking dict.

```python
def __init__(self, ...):
    # ... existing initialization ...
    
    # Plan type tracking (separate from TradePlan dataclass)
    # NOTE: These are accessed only from monitor thread, so no lock needed
    # If accessed from other threads in future, add locks
    self._plan_types: Dict[str, str] = {}  # plan_id -> plan_type
    self._plan_last_check: Dict[str, datetime] = {}  # plan_id -> last check time (UTC datetime)
    self._plan_last_price: Dict[str, float] = {}  # plan_id -> last known price
    
    # M1 cache invalidation tracking (for candle-close detection)
    # CRITICAL: Initialize here, not in method (avoids hasattr check every time)
    self._m1_latest_candle_times: Dict[str, datetime] = {}  # symbol (normalized) -> latest candle time (UTC datetime)
    
    # Pre-fetch thread reference (initialized in start() method)
    # NOTE: Import Optional from typing if not already imported
    self.prefetch_thread: Optional[threading.Thread] = None
    
    # Optional: Add locks if accessed from multiple threads in future
    # self._plan_tracking_lock = threading.Lock()  # For thread safety if needed
```

```python
def _detect_plan_type(self, plan: TradePlan) -> str:
    """
    Detect plan type based on conditions and timeframe.
    
    Returns:
        'm1_micro_scalp', 'm5_range_scalp', 'm15_range_scalp', or 'default'
    """
    # Check cache first
    if plan.plan_id in self._plan_types:
        return self._plan_types[plan.plan_id]
    
    timeframe = plan.conditions.get('timeframe', '').upper()
    
    # M1 micro-scalp detection
    if timeframe == 'M1':
        # Check for micro-scalp indicators
        has_liquidity_sweep = plan.conditions.get('liquidity_sweep', False)
        has_order_block = plan.conditions.get('order_block', False)
        has_equal_lows = plan.conditions.get('equal_lows', False) or plan.conditions.get('equal_highs', False)
        has_vwap_deviation = plan.conditions.get('vwap_deviation', False)
        
        # M1 with micro-scalp conditions
        if has_liquidity_sweep or has_order_block or has_equal_lows or has_vwap_deviation:
            plan_type = 'm1_micro_scalp'
            self._plan_types[plan.plan_id] = plan_type
            return plan_type
    
    # M5 range scalp detection
    if timeframe == 'M5':
        has_range_scalp = plan.conditions.get('range_scalp_confluence') is not None
        has_vwap_deviation = plan.conditions.get('vwap_deviation', False)
        has_mean_reversion = 'mean_reversion' in str(plan.notes or '').lower()
        
        if has_range_scalp or (has_vwap_deviation and has_mean_reversion):
            plan_type = 'm5_range_scalp'
            self._plan_types[plan.plan_id] = plan_type
            return plan_type
    
    # M15 range scalp detection
    if timeframe == 'M15':
        has_range_scalp = plan.conditions.get('range_scalp_confluence') is not None
        if has_range_scalp:
            plan_type = 'm15_range_scalp'
            self._plan_types[plan.plan_id] = plan_type
            return plan_type
    
    # Default
    plan_type = 'default'
    self._plan_types[plan.plan_id] = plan_type
    return plan_type
```

**Action Items:**
- [x] Add `_plan_types`, `_plan_last_check`, `_plan_last_price` dicts to `__init__`
- [x] Implement `_detect_plan_type()` method
- [x] Cache plan types to avoid re-detection
- [x] **CRITICAL:** Clean up tracking dicts when plans are removed (in `_cleanup_plan_resources()`)
- [ ] Add unit tests

**Cleanup Integration:** Add to existing `_cleanup_plan_resources()` method (around line 1390):

```python
def _cleanup_plan_resources(self, plan_id: str, symbol: str):
    """Clean up resources for a plan"""
    # ... existing cleanup code ...
    
    # NEW: Clean up optimized intervals tracking dicts
    # NOTE: These are accessed only from monitor thread, so no lock needed
    # But use hasattr checks for safety in case dicts aren't initialized
    try:
        if hasattr(self, '_plan_types') and plan_id in self._plan_types:
            del self._plan_types[plan_id]
        if hasattr(self, '_plan_last_check') and plan_id in self._plan_last_check:
            del self._plan_last_check[plan_id]
        if hasattr(self, '_plan_last_price') and plan_id in self._plan_last_price:
            del self._plan_last_price[plan_id]
    except Exception as e:
        logger.debug(f"Error cleaning up tracking dicts for {plan_id}: {e}")
```

---

### **Phase 2: Adaptive Intervals (Day 2)**

#### **Step 2.1: Implement Adaptive Interval Logic**

**File:** `auto_execution_system.py`

**Changes:**
1. Add method to calculate adaptive interval per plan
2. Modify `_monitor_loop()` to use adaptive intervals
3. Track price proximity for each plan

**Code Location:** In `AutoExecutionSystem` class

```python
def _calculate_adaptive_interval(self, plan: TradePlan, current_price: float) -> int:
    """
    Calculate adaptive check interval based on plan type and price proximity.
    
    Args:
        plan: Trade plan
        current_price: Current market price
        
    Returns:
        Interval in seconds (defaults to self.check_interval on error)
    """
    try:
        # Check if adaptive intervals are enabled
        opt_config = self.config.get('optimized_intervals', {})
        if not opt_config.get('adaptive_intervals', {}).get('enabled', False):
            return self.check_interval
        
        # Get plan type (cached)
        plan_type = self._plan_types.get(plan.plan_id) or self._detect_plan_type(plan)
        interval_config = opt_config.get('adaptive_intervals', {}).get('plan_type_intervals', {}).get(plan_type, {})
        
        if not interval_config:
            return self.check_interval
        
        # Calculate price distance from entry
        entry_price = plan.entry_price
        tolerance = plan.conditions.get('tolerance', 0)
        
        if tolerance == 0:
            # No tolerance specified - use base interval
            return interval_config.get('base_interval_seconds', self.check_interval)
        
        price_distance = abs(current_price - entry_price)
        proximity_multiplier = interval_config.get('price_proximity_multiplier', 2.0)
        proximity_threshold = tolerance * proximity_multiplier
        
        # Determine interval based on proximity
        if price_distance <= tolerance:
            # Price is within tolerance - use close interval (fastest)
            return interval_config.get('close_interval_seconds', interval_config.get('base_interval_seconds', 10))
        elif price_distance <= proximity_threshold:
            # Price is within 2Ã— tolerance - use base interval
            return interval_config.get('base_interval_seconds', 30)
        else:
            # Price is far - use far interval (slowest)
            return interval_config.get('far_interval_seconds', 60)
    except Exception as e:
        logger.debug(f"Error calculating adaptive interval for {plan.plan_id}: {e}")
        # Return default interval on error
        return self.check_interval
```

**Action Items:**
- [ ] Implement `_calculate_adaptive_interval()` method
- [ ] Add price tracking per plan
- [ ] Modify `_monitor_loop()` to use adaptive intervals
- [ ] Add logging for interval changes

---

#### **Step 2.2: Modify Monitor Loop**

**File:** `auto_execution_system.py`

**Changes:**
1. Group plans by calculated interval
2. Check plans at their respective intervals
3. Track last check time per plan

**Code Location:** `_monitor_loop()` method

```python
def _monitor_loop(self):
    """Main monitoring loop with adaptive intervals (INTEGRATED with existing logic)"""
    thread_name = threading.current_thread().name
    logger.info(f"Auto execution system monitoring loop started (thread: {thread_name})")
    
    try:
        while self.running:
            try:
                # ========================================================================
                # EXISTING LOGIC: Batch refresh M1 data, cache cleanup, plan reload
                # ========================================================================
                # ... keep all existing logic from lines 5812-5915 ...
                
                # ========================================================================
                # EXISTING LOGIC: Get plans to check (with lock) - MUST BE BEFORE price fetch
                # ========================================================================
                try:
                    with self.plans_lock:
                        if self.plans is None:
                            logger.warning("self.plans is None, reinitializing...")
                            self.plans = {}
                        plans_to_check = list(self.plans.items())
                except Exception as e:
                    logger.error(f"Error acquiring plans lock: {e}", exc_info=True)
                    time.sleep(self.check_interval)
                    continue
                
                if plans_to_check is None:
                    plans_to_check = []
                
                # ========================================================================
                # NEW: Get current prices for all symbols (batch) - AFTER getting plans
                # OPTIMIZATION: Only fetch if there are pending plans and features enabled
                # ========================================================================
                opt_config = self.config.get('optimized_intervals', {})
                use_adaptive = opt_config.get('adaptive_intervals', {}).get('enabled', False)
                use_conditional = opt_config.get('conditional_checks', {}).get('enabled', False)
                
                symbol_prices = {}
                if (use_adaptive or use_conditional) and plans_to_check:
                    try:
                        symbol_prices = self._get_current_prices_batch()
                    except Exception as e:
                        logger.warning(f"Error fetching batch prices (non-fatal): {e}")
                        # Continue with empty dict - plans will use last known prices or skip adaptive checks
                        symbol_prices = {}
                
                # ========================================================================
                # MODIFIED: Check each plan with adaptive intervals and conditional checks
                # ========================================================================
                now_utc = datetime.now(timezone.utc)
                
                for plan_id, plan in plans_to_check:
                    # ... keep all existing validation checks (status, expiration, etc.) ...
                    
                    if plan.status != "pending":
                        continue
                    
                    # ... keep all existing expiration checks (lines 5961-6010) ...
                    # ... keep all existing cancellation checks (lines 6012-6045) ...
                    # ... keep all existing re-evaluation checks (lines 6047-6063) ...
                    
                    # ====================================================================
                    # NEW: Adaptive interval check - AFTER expiration/cancellation checks
                    # CRITICAL: Must happen AFTER expiration/cancellation but BEFORE expensive checks
                    # ====================================================================
                    should_skip_due_to_interval = False
                    if use_adaptive:
                        try:
                            symbol_norm = plan.symbol.upper().rstrip('Cc') + 'c'
                            current_price = symbol_prices.get(symbol_norm)
                            
                            if current_price is not None:
                                # Calculate adaptive interval
                                interval = self._calculate_adaptive_interval(plan, current_price)
                                
                                # Check if enough time has passed since last check
                                last_check = self._plan_last_check.get(plan_id)
                                if last_check:
                                    time_since_check = (now_utc - last_check).total_seconds()
                                    if time_since_check < interval:
                                        # Not enough time passed - skip this check
                                        should_skip_due_to_interval = True
                                
                                # Store current price for next iteration (even if skipping)
                                self._plan_last_price[plan_id] = current_price
                            else:
                                # Price not available - use last known price if available
                                current_price = self._plan_last_price.get(plan_id)
                                if current_price is None:
                                    # No price available - skip adaptive check, continue with normal flow
                                    pass
                        except Exception as e:
                            logger.debug(f"Error in adaptive interval check for {plan_id}: {e}")
                            # Continue with normal check if adaptive check fails
                    
                    if should_skip_due_to_interval:
                        continue  # Skip this plan - not enough time passed
                    
                    # ====================================================================
                    # NEW: Conditional check - skip if price is too far from entry
                    # ====================================================================
                    if use_conditional:
                        try:
                            symbol_norm = plan.symbol.upper().rstrip('Cc') + 'c'
                            current_price = symbol_prices.get(symbol_norm) or self._plan_last_price.get(plan_id)
                            
                            if current_price is not None:
                                if not self._should_check_plan(plan, current_price):
                                    # Price too far - skip check
                                    # NOTE: Don't update last_check time since we skipped
                                    continue
                        except Exception as e:
                            logger.debug(f"Error in conditional check for {plan_id}: {e}")
                            # Continue with normal check if conditional check fails
                    
                    # ====================================================================
                    # EXISTING LOGIC: All other checks (M1 staleness, M1 refresh, etc.)
                    # ====================================================================
                    # ... keep ALL existing checks from lines 6065-6098 ...
                    
                    # ====================================================================
                    # EXISTING LOGIC: Check conditions and execute
                    # ====================================================================
                    plan_was_executed = False
                    try:
                        if self._check_conditions(plan):
                            logger.info(f"Conditions met for plan {plan_id}, executing trade")
                            try:
                                if self._execute_trade(plan):
                                    # Execution successful - remove from memory and clear failure count
                                    try:
                                        with self.plans_lock:
                                            if plan_id in self.plans:
                                                del self.plans[plan_id]
                                                plan_was_executed = True  # Plan was removed
                                    except Exception as e:
                                        logger.warning(f"Error removing plan {plan_id} from memory: {e}")
                                    
                                    if plan_id in self.execution_failures:
                                        del self.execution_failures[plan_id]
                                    # Clear symbol failure count on successful execution
                                    plan_symbol = getattr(plan, 'symbol', None)
                                    if plan_symbol and plan_symbol in self.invalid_symbols:
                                        del self.invalid_symbols[plan_symbol]
                                    # Clean up execution locks and other resources
                                    self._cleanup_plan_resources(plan_id, plan_symbol or 'unknown')
                                # else: Execution failed - plan still exists, continue monitoring
                            except Exception as e:
                                logger.error(f"Error executing trade for plan {plan_id}: {e}", exc_info=True)
                                # Execution error - plan still exists, continue monitoring
                    except Exception as e:
                        logger.error(f"Error checking conditions for plan {plan_id}: {e}", exc_info=True)
                        # Continue to next plan - condition check failure shouldn't kill the thread
                    
                    # ====================================================================
                    # NEW: Update last check time - ONLY if we actually checked the plan
                    # CRITICAL: Only update if:
                    #   1. We didn't skip due to interval or conditional check
                    #   2. Plan still exists (wasn't removed due to successful execution)
                    # ====================================================================
                    if use_adaptive and not plan_was_executed:
                        # Only update if we actually performed the check AND plan still exists
                        # (If plan was executed and removed, _cleanup_plan_resources will clean up tracking dicts)
                        try:
                            with self.plans_lock:
                                if plan_id in self.plans:
                                    self._plan_last_check[plan_id] = now_utc
                        except Exception as e:
                            logger.debug(f"Error updating last check time for {plan_id}: {e}")
                            # Non-critical - continue
                
                # ========================================================================
                # EXISTING LOGIC: Sleep before next check
                # ========================================================================
                # ... keep existing sleep logic from lines 6159-6177 ...
                # NOTE: With adaptive intervals, we could sleep for minimum interval instead of
                # fixed check_interval, but this adds complexity. For now, keep existing sleep
                # logic (sleeps for check_interval). The adaptive interval check will handle
                # skipping plans that don't need checking yet.
                
            except KeyboardInterrupt:
                logger.info("Monitor loop received KeyboardInterrupt, stopping...")
                self.running = False
                break
            except Exception as e:
                # ... keep existing error handling ...
                logger.error(f"Error in monitoring loop: {e}", exc_info=True)
                time.sleep(self.check_interval)
    except Exception as e:
        logger.error(f"Fatal error in monitor loop: {e}", exc_info=True)
```

**Action Items:**
- [x] **CRITICAL:** Preserve ALL existing monitor loop logic (expiration, re-evaluation, M1 refresh, etc.)
- [x] **CRITICAL:** Add adaptive interval check AFTER expiration/cancellation checks but BEFORE expensive condition checks
- [x] **CRITICAL:** Only update `_plan_last_check` if plan was actually checked (not skipped)
- [x] Add conditional check (price proximity) AFTER adaptive interval check
- [x] Add error handling for `_get_current_prices_batch()` failures
- [x] Add error handling for `_calculate_adaptive_interval()` failures
- [x] Optimize: Only fetch prices if there are pending plans
- [x] Add `_get_current_prices_batch()` method
- [ ] Test interval logic without breaking existing functionality
- [x] Ensure graceful degradation if config disabled or price fetch fails
- [x] **NOTE:** Tracking dicts accessed only from monitor thread - no locks needed (add if accessed from other threads)

---

### **Phase 3: Smart Caching (Day 3)**

#### **Step 3.1: Implement Candle-Close Cache Invalidation**

**File:** `auto_execution_system.py` and `infra/m1_data_fetcher.py`

**Changes:**
1. Track M1 candle close times
2. Invalidate cache on candle close (not time-based)
3. Pre-fetch data before cache expiry

**Code Location:** New method in `AutoExecutionSystem` class, call from `_monitor_loop()` before M1 refresh

**IMPORTANT:** Use existing `_m1_data_cache` and `_m1_cache_timestamps` structures, not non-existent `_cache`.

```python
def _invalidate_cache_on_candle_close(self, symbol: str, timeframe: str = 'M1'):
    """
    Invalidate M1 cache when new candle closes.
    
    Args:
        symbol: Symbol to check
        timeframe: Timeframe (M1 only for now)
    """
    if timeframe != 'M1' or not self.m1_data_fetcher:
        return
    
    opt_config = self.config.get('optimized_intervals', {})
    if not opt_config.get('smart_caching', {}).get('invalidate_on_candle_close', False):
        return
    
    try:
        # Normalize symbol for consistency
        symbol_base = symbol.upper().rstrip('Cc')
        symbol_norm = symbol_base + 'c'
        
        # Get latest candle time (without cache)
        candles = self.m1_data_fetcher.fetch_m1_data(symbol_norm, count=1, use_cache=False)
        if not candles or len(candles) == 0:
            return
        
        latest_candle = candles[-1]
        latest_candle_time = latest_candle.get('time')
        
        if not latest_candle_time:
            return
        
        # Normalize symbol for consistency
        symbol_base = symbol.upper().rstrip('Cc')
        symbol_norm = symbol_base + 'c'
        
        # Check if this is a new candle (different from cached)
        # Store latest candle time in a separate tracking dict (not in _m1_cache_timestamps)
        # _m1_cache_timestamps stores cache timestamps as float (time.time())
        # NOTE: _m1_latest_candle_times should be initialized in __init__ (not checked here)
        
        # Convert latest_candle_time to comparable format (datetime)
        if isinstance(latest_candle_time, datetime):
            latest_time = latest_candle_time
            # Ensure UTC-aware
            if latest_time.tzinfo is None:
                latest_time = latest_time.replace(tzinfo=timezone.utc)
        elif isinstance(latest_candle_time, (int, float)):
            # CRITICAL: MT5 timestamps are UTC, must use tz=timezone.utc
            latest_time = datetime.fromtimestamp(latest_candle_time, tz=timezone.utc)
        else:
            logger.debug(f"Unknown candle time format for {symbol_norm}: {type(latest_candle_time)}")
            return  # Unknown format
        
        cached_time = self._m1_latest_candle_times.get(symbol_norm)
        
        if cached_time != latest_time:
            # New candle - invalidate M1 cache (use normalized symbol)
            if symbol_norm in self._m1_data_cache:
                del self._m1_data_cache[symbol_norm]
            if symbol_norm in self._m1_cache_timestamps:
                del self._m1_cache_timestamps[symbol_norm]
            
            # Store new candle time (use normalized symbol)
            self._m1_latest_candle_times[symbol_norm] = latest_time
            logger.debug(f"Invalidated M1 cache for {symbol_norm} due to new candle close")
    except Exception as e:
        logger.debug(f"Error checking candle close for {symbol}: {e}")
```

**Action Items:**
- [x] **CRITICAL:** Use existing `_m1_data_cache` and `_m1_cache_timestamps` structures
- [x] Implement candle-close detection using M1 data fetcher
- [x] Add cache invalidation on candle close (modify existing cache, don't create new)
- [x] Call from `_monitor_loop()` before M1 batch refresh
- [ ] Test cache invalidation with existing cache structure

---

#### **Step 3.2: Implement Pre-fetching**

**File:** `auto_execution_system.py`

**Changes:**
1. Pre-fetch data 2-3 seconds before cache expiry
2. Background thread for pre-fetching
3. Reduce cache misses

**Code Location:** New method in `AutoExecutionSystem`

```python
def _prefetch_data_before_expiry(self):
    """
    Pre-fetch M1 data for plans before cache expires.
    Runs in background thread.
    """
    opt_config = self.config.get('optimized_intervals', {})
    if not opt_config.get('smart_caching', {}).get('enabled', False):
        return
    
    prefetch_seconds = opt_config.get('smart_caching', {}).get('prefetch_seconds_before_expiry', 3)
    cache_ttl = opt_config.get('smart_caching', {}).get('m1_cache_ttl_seconds', 20)
    
    while self.running:
        try:
            # Get symbols with active plans
            active_symbols = set()
            with self.plans_lock:
                for plan in self.plans.values():
                    if plan.status == 'pending':
                        symbol_norm = plan.symbol.upper().rstrip('Cc') + 'c'
                        active_symbols.add(symbol_norm)
            
            # Check cache expiry times using existing _m1_cache_timestamps
            # _m1_cache_timestamps stores float timestamps (time.time())
            now_time = time.time()
            
            for symbol in active_symbols:
                if symbol not in self._m1_cache_timestamps:
                    continue
                
                # Get cache timestamp (stored as float from time.time())
                cache_timestamp = self._m1_cache_timestamps.get(symbol)
                
                if not isinstance(cache_timestamp, (int, float)):
                    continue
                
                # Calculate time until expiry
                cache_age = now_time - cache_timestamp
                time_until_expiry = cache_ttl - cache_age
                
                if 0 < time_until_expiry <= prefetch_seconds:
                    # Pre-fetch data
                    logger.debug(f"Pre-fetching M1 data for {symbol} ({time_until_expiry:.1f}s until expiry)")
                    try:
                        if self.m1_data_fetcher:
                            self.m1_data_fetcher.fetch_m1_data(symbol, count=200, use_cache=False)
                        else:
                            logger.debug(f"M1 data fetcher not available for {symbol}")
                    except Exception as prefetch_error:
                        logger.debug(f"Error pre-fetching M1 data for {symbol}: {prefetch_error}")
                        # Continue with other symbols - pre-fetch failure is non-critical
            
            # Sleep for 1 second
            time.sleep(1)
            
        except Exception as e:
            logger.debug(f"Error in pre-fetch thread: {e}")
            time.sleep(5)
```

**Integration:** Add pre-fetch thread start in `start()` method:

```python
def start(self):
    """Start the monitoring system"""
    if self.running:
        logger.warning("Auto execution system is already running")
        return
    
    self.running = True
    
    # Start main monitor thread
    self.monitor_thread = threading.Thread(
        target=self._monitor_loop,
        name="AutoExecutionMonitor",
        daemon=True
    )
    self.monitor_thread.start()
    
    # Start pre-fetch thread if enabled
    opt_config = self.config.get('optimized_intervals', {})
    if opt_config.get('smart_caching', {}).get('enabled', False):
        self.prefetch_thread = threading.Thread(
            target=self._prefetch_data_before_expiry,
            name="AutoExecutionPrefetch",
            daemon=True  # Daemon thread - will exit when main thread exits
        )
        self.prefetch_thread.start()
        logger.info("Pre-fetch thread started")
```

**Integration with `stop()` method (around line 6311):**
- Pre-fetch thread will automatically exit when `self.running = False` (checked in `_prefetch_data_before_expiry()` loop)
- Optional: Add cleanup in `stop()` method:
```python
def stop(self):
    """Stop the auto execution system"""
    logger.info("Stopping auto execution system...")
    self.running = False
    
    # ... existing stop logic ...
    
    # NEW: Stop pre-fetch thread if running
    if hasattr(self, 'prefetch_thread') and self.prefetch_thread and self.prefetch_thread.is_alive():
        logger.debug("Waiting for pre-fetch thread to finish...")
        try:
            self.prefetch_thread.join(timeout=2.0)
        except Exception as e:
            logger.debug(f"Error joining pre-fetch thread: {e}")
```
```

**Action Items:**
- [ ] **CRITICAL:** Use existing `_m1_cache_timestamps` structure (not non-existent `_cache_expiry`)
- [ ] Implement pre-fetching logic with existing cache structures
- [ ] Add background thread start in `start()` method
- [ ] Add thread cleanup in `stop()` method
- [ ] Test pre-fetching effectiveness
- [ ] Monitor cache hit rate improvement

---

### **Phase 4: Conditional Checks (Day 4)**

#### **Step 4.1: Implement Price Proximity Filter**

**File:** `auto_execution_system.py`

**Changes:**
1. Skip condition checks when price is far from entry
2. Only check when price is within 2Ã— tolerance
3. Reduce unnecessary checks by 40-60%

**Code Location:** In `_monitor_loop()` before condition checks

```python
def _should_check_plan(self, plan: TradePlan, current_price: float) -> bool:
    """
    Determine if plan should be checked based on price proximity.
    
    Args:
        plan: Trade plan
        current_price: Current market price
        
    Returns:
        True if plan should be checked, False otherwise
    """
    opt_config = self.config.get('optimized_intervals', {})
    conditional_config = opt_config.get('conditional_checks', {})
    
    if not conditional_config.get('enabled', False):
        return True
    
    if not conditional_config.get('price_proximity_filter', False):
        return True
    
    # Calculate price distance from entry
    entry_price = plan.entry_price
    tolerance = plan.conditions.get('tolerance', 0)
    
    if tolerance == 0:
        return True  # No tolerance specified, always check
    
    price_distance = abs(current_price - entry_price)
    proximity_multiplier = conditional_config.get('proximity_multiplier', 2.0)
    proximity_threshold = tolerance * proximity_multiplier
    
    # Only check if price is within 2Ã— tolerance
    if price_distance <= proximity_threshold:
        return True
    
    # Price is too far - skip check
    logger.debug(f"Skipping check for {plan.plan_id}: price {current_price:.2f} is {price_distance:.2f} away from entry {entry_price:.2f} (threshold: {proximity_threshold:.2f})")
    return False
```

**Action Items:**
- [ ] Implement `_should_check_plan()` method
- [ ] Add to `_monitor_loop()` before condition checks
- [ ] Add logging for skipped checks
- [ ] Test proximity filter effectiveness

---

### **Phase 5: Batch Operations (Day 5)**

#### **Step 5.1: Batch MT5 API Calls**

**File:** `auto_execution_system.py`

**Changes:**
1. Group MT5 API calls per cycle
2. Batch price quotes
3. Batch candle fetches

**Code Location:** New method in `AutoExecutionSystem`

```python
def _get_current_prices_batch(self) -> Dict[str, float]:
    """
    Get current prices for all active symbols in one batch.
    
    Returns:
        Dictionary mapping symbol to current price (mid price)
    """
    prices = {}
    
    # Get unique symbols from active plans (with lock)
    active_symbols = set()
    try:
        with self.plans_lock:
            for plan in self.plans.values():
                if plan.status == 'pending':
                    symbol_norm = plan.symbol.upper().rstrip('Cc') + 'c'
                    active_symbols.add(symbol_norm)
    except Exception as e:
        logger.warning(f"Error getting active symbols: {e}")
        return prices
    
    if not active_symbols:
        return prices
    
    # Batch fetch quotes using MT5Service
    if self.mt5_service:
        try:
            if not self.mt5_service.connect():
                logger.warning("MT5 not connected, cannot fetch prices")
                return prices
            
            for symbol in active_symbols:
                try:
                    # MT5Service.get_quote() returns Quote dataclass with bid/ask attributes
                    quote = self.mt5_service.get_quote(symbol)
                    if quote:
                        # Quote is a dataclass with bid and ask attributes
                        try:
                            # Use mid price
                            prices[symbol] = (quote.bid + quote.ask) / 2
                        except (AttributeError, TypeError) as e:
                            logger.debug(f"Error accessing quote attributes for {symbol}: {e}")
                            # Fallback: try symbol_info_tick
                            tick = mt5.symbol_info_tick(symbol)
                            if tick:
                                prices[symbol] = (tick.bid + tick.ask) / 2
                    else:
                        # Quote is None - fallback to symbol_info_tick
                        tick = mt5.symbol_info_tick(symbol)
                        if tick:
                            prices[symbol] = (tick.bid + tick.ask) / 2
                except Exception as e:
                    logger.debug(f"Error getting quote for {symbol}: {e}")
                    # Continue with other symbols
        except Exception as e:
            logger.warning(f"Error in batch price fetch: {e}")
    
    return prices
```

**Action Items:**
- [ ] **CRITICAL:** Verify MT5Service.get_quote() API exists and returns Quote object
- [ ] Implement `_get_current_prices_batch()` method with proper error handling
- [ ] Add fallback to mt5.symbol_info_tick() if get_quote() fails
- [ ] Use plans_lock when accessing self.plans
- [ ] Test batch operation performance
- [ ] Monitor API call reduction

---

#### **Step 5.2: Batch Database Operations**

**File:** `auto_execution_system.py`

**Changes:**
1. Batch database reads
2. Batch database writes
3. Use write queue for writes

**Code Location:** In database operations

```python
def _batch_update_plan_statuses(self, updates: List[Tuple[str, str]]):
    """
    Batch update plan statuses in database.
    
    Args:
        updates: List of (plan_id, status) tuples
    """
    if not updates:
        return
    
    try:
        if self.db_write_queue:
            # Use write queue for batching (verify API exists)
            from infra.database_write_queue import OperationPriority
            
            for plan_id, status in updates:
                try:
                    self.db_write_queue.add_operation(
                        operation='update_plan_status',
                        data={'plan_id': plan_id, 'status': status},
                        priority=OperationPriority.NORMAL  # Use enum if available
                    )
                except Exception as e:
                    logger.debug(f"Error adding update to queue for {plan_id}: {e}")
                    # Fallback to direct update for this plan
                    try:
                        with sqlite3.connect(self.db_path, timeout=5.0) as conn:
                            conn.execute(
                                "UPDATE trade_plans SET status = ? WHERE plan_id = ?",
                                (status, plan_id)
                            )
                            conn.commit()
                    except Exception as e2:
                        logger.error(f"Error direct updating plan {plan_id}: {e2}")
        else:
            # Fallback to direct database update (batch)
            with sqlite3.connect(self.db_path, timeout=10.0) as conn:
                conn.executemany(
                    "UPDATE trade_plans SET status = ? WHERE plan_id = ?",
                    [(status, plan_id) for plan_id, status in updates]
                )
                conn.commit()
    except Exception as e:
        logger.error(f"Error batch updating plan statuses: {e}", exc_info=True)
```

**Note:** This method is optional - existing `_update_plan_status()` already uses write queue. Only needed if batching multiple updates at once.

**Action Items:**
- [ ] **NOTE:** Existing `_update_plan_status()` already uses write queue - this is optional
- [ ] Verify database_write_queue API (OperationPriority enum, add_operation method)
- [ ] Implement batch database operations only if needed
- [ ] Test batch operation performance
- [ ] Monitor database operation reduction
- [ ] **Alternative:** Keep using existing `_update_plan_status()` per plan (already efficient)

---

### **Phase 6: Testing & Validation (Day 6-7)**

#### **Step 6.1: Unit Tests**

**Test Files:**
- `tests/test_adaptive_intervals.py`
- `tests/test_smart_caching.py`
- `tests/test_conditional_checks.py`
- `tests/test_batch_operations.py`

**Test Cases:**
1. Adaptive interval calculation
2. Price proximity filter
3. Cache invalidation on candle close
4. Pre-fetching logic
5. Batch operations
6. Plan type detection

**Action Items:**
- [ ] Create unit tests
- [ ] Run test suite
- [ ] Fix any failures
- [ ] Achieve >90% code coverage

---

#### **Step 6.2: Integration Tests**

**Test Scenarios:**
1. M1 micro-scalp plan with adaptive intervals
2. Price proximity filter skipping checks
3. Cache invalidation on M1 candle close
4. Batch operations reducing API calls
5. Resource usage monitoring

**Action Items:**
- [ ] Create integration tests
- [ ] Run integration test suite
- [ ] Validate resource usage
- [ ] Verify cache hit rate improvement

---

#### **Step 6.3: Performance Testing**

**Metrics to Monitor:**
- CPU usage (target: 8-18%)
- RAM usage (target: +10-20 MB)
- Cache hit rate (target: 70-80%)
- MT5 API calls (target: 20-40 calls/min)
- Check reduction (target: 40-60%)
- Missed trade reduction (target: 50-70%)

**Action Items:**
- [ ] Set up performance monitoring
- [ ] Run performance tests
- [ ] Compare before/after metrics
- [ ] Validate targets are met

---

### **Phase 7: Rollout (Day 8)**

#### **Step 7.1: Gradual Rollout**

**Rollout Strategy:**
1. **Day 1:** Enable for 1 M1 micro-scalp plan (test)
2. **Day 2:** Enable for all XAUUSDc M1 plans
3. **Day 3:** Enable for all BTCUSDc M1 plans
4. **Day 4:** Full rollout if no issues

**Action Items:**
- [ ] Create feature flag in config
- [ ] Enable for test plan
- [ ] Monitor for 24 hours
- [ ] Gradually expand rollout
- [ ] Full rollout if successful

---

#### **Step 7.2: Monitoring & Alerts**

**Monitoring:**
- Resource usage (CPU, RAM, SSD)
- Cache hit rate
- MT5 API call rate
- Check reduction percentage
- Missed trade tracking

**Alerts:**
- CPU usage > 25%
- Cache hit rate < 60%
- MT5 API errors
- Check reduction < 30%

**Action Items:**
- [ ] Set up monitoring dashboards
- [ ] Configure alerts
- [ ] Create monitoring reports
- [ ] Review daily for first week

---

## ðŸ“Š **Success Metrics**

### **Performance Targets**

| Metric | Current (30s) | Target (10s Optimized) | Status |
|--------|---------------|----------------------|--------|
| **CPU Usage** | 4-10% | 8-18% | âš ï¸ To be validated |
| **RAM Usage** | 31-62 MB | +10-20 MB | âš ï¸ To be validated |
| **Cache Hit Rate** | 50-60% | 70-80% | âš ï¸ To be validated |
| **MT5 API Calls** | 10-20/min | 20-40/min | âš ï¸ To be validated |
| **Check Reduction** | 0% | 40-60% | âš ï¸ To be validated |
| **Missed Trades** | Baseline | -50-70% | âš ï¸ To be validated |

---

## âš ï¸ **Risks & Mitigations**

### **Risk 1: Breaking Existing Monitor Loop Logic**

**Issue:** Monitor loop has complex logic (expiration, re-evaluation, M1 refresh, etc.)

**Mitigation:**
- âœ… **CRITICAL:** Preserve ALL existing logic
- âœ… Add new checks BEFORE existing condition checks (early return)
- âœ… Use feature flags (disabled by default)
- âœ… Comprehensive integration tests
- âœ… Gradual rollout

---

### **Risk 2: CPU Usage Higher Than Expected**

**Mitigation:**
- Monitor CPU usage closely
- Implement additional optimizations if needed
- Consider reducing to 15 seconds if 10s is too aggressive
- Use conditional checks to reduce unnecessary work

---

### **Risk 3: Cache Structure Mismatch**

**Issue:** Plan references non-existent `_cache` and `_cache_expiry`

**Mitigation:**
- âœ… **FIXED:** Use existing `_m1_data_cache` and `_m1_cache_timestamps`
- âœ… Verify cache structure before implementing
- âœ… Test cache operations with existing structures

---

### **Risk 4: TradePlan Dataclass Modification**

**Issue:** Plan assumes modifying TradePlan dataclass directly

**Mitigation:**
- âœ… **FIXED:** Use separate tracking dicts (`_plan_types`, `_plan_last_check`)
- âœ… Do NOT modify TradePlan dataclass
- âœ… Clean up tracking dicts when plans removed

---

### **Risk 5: Config Loading Integration**

**Issue:** Plan assumes JSON file loading, but system uses dict structure

**Mitigation:**
- âœ… **FIXED:** Integrate with existing `self.config` dict
- âœ… Load from JSON if exists, otherwise use defaults
- âœ… Feature disabled by default (safe)

---

### **Risk 6: MT5 API Rate Limiting**

**Mitigation:**
- Monitor API call rate
- Implement exponential backoff
- Reduce batch size if needed
- Verify MT5Service.get_quote() API exists

---

### **Risk 7: Thread Management**

**Issue:** Adding pre-fetch thread needs proper lifecycle management

**Mitigation:**
- âœ… Start thread in `start()` method
- âœ… Stop thread in `stop()` method
- âœ… Use daemon threads
- âœ… Handle thread exceptions gracefully

---

### **Risk 8: Check Order Logic**

**Issue:** Adaptive interval check might skip plans that should be expired/cancelled

**Mitigation:**
- âœ… **FIXED:** Adaptive interval check happens AFTER expiration/cancellation checks
- âœ… Expiration/cancellation checks always run first
- âœ… Adaptive check only skips expensive condition checks, not critical status checks

---

### **Risk 9: Last Check Time Update Logic**

**Issue:** Last check time updated even when plan was skipped

**Mitigation:**
- âœ… **FIXED:** Only update `_plan_last_check` if plan was actually checked
- âœ… Use flag `should_skip_due_to_interval` to track skip state
- âœ… Update timestamp only after all checks complete (not if skipped)

---

### **Risk 10: Price Fetch Failure**

**Issue:** If price fetch fails, all plans might be affected

**Mitigation:**
- âœ… **FIXED:** Added error handling for `_get_current_prices_batch()` failures
- âœ… Graceful degradation - use last known price or skip adaptive checks
- âœ… Continue with normal flow if price fetch fails
- âœ… Only fetch prices if there are pending plans (optimization)

---

### **Risk 11: Dict Initialization**

**Issue:** `_m1_latest_candle_times` checked with `hasattr` every time (inefficient)

**Mitigation:**
- âœ… **FIXED:** Initialize `_m1_latest_candle_times` in `__init__` (not in method)
- âœ… Avoids hasattr check on every call
- âœ… Initialize `prefetch_thread = None` in `__init__` for proper cleanup

---

### **Risk 12: Symbol Normalization Inconsistency**

**Issue:** Symbol normalization might be inconsistent across methods

**Mitigation:**
- âœ… **FIXED:** Added symbol normalization in `_invalidate_cache_on_candle_close()`
- âœ… Use consistent normalization pattern: `symbol.upper().rstrip('Cc') + 'c'`
- âœ… Ensures cache keys match across all methods

---

### **Risk 13: Candle Time Format Handling**

**Issue:** Candle time might be in different formats (datetime, int, float)

**Mitigation:**
- âœ… **FIXED:** Added proper handling for all formats
- âœ… Ensure UTC timezone awareness for datetime objects
- âœ… Use `datetime.fromtimestamp(..., tz=timezone.utc)` for int/float timestamps
- âœ… Log warning for unknown formats

---

### **Risk 14: Pre-fetch Thread Error Handling**

**Issue:** Pre-fetch failures might crash the thread or affect other symbols

**Mitigation:**
- âœ… **FIXED:** Added try/except around pre-fetch operations
- âœ… Continue with other symbols if one fails
- âœ… Log errors but don't crash thread
- âœ… Pre-fetch failures are non-critical

---

### **Risk 15: Config Merge Logic**

**Issue:** `self.config.update()` does shallow merge - might overwrite nested dicts incorrectly

**Mitigation:**
- âœ… **FIXED:** Use deep merge for nested `optimized_intervals` dict
- âœ… Preserves existing config values when merging
- âœ… Handles case where JSON has `optimized_intervals` key vs. top-level keys

---

### **Risk 16: Order of Operations**

**Issue:** `plans_to_check` used before it's defined (fetching prices before getting plans)

**Mitigation:**
- âœ… **FIXED:** Get plans FIRST, then fetch prices
- âœ… Prevents undefined variable error
- âœ… More efficient - only fetch prices if there are plans to check

---

### **Risk 17: Quote Dataclass Access**

**Issue:** Quote dataclass attribute access might fail if structure changes

**Mitigation:**
- âœ… **FIXED:** Added try/except around quote.bid/ask access
- âœ… Fallback to `mt5.symbol_info_tick()` if quote access fails
- âœ… Handles None quote gracefully

---

### **Risk 18: Plan Removal After Execution**

**Issue:** `_plan_last_check` updated after plan might have been removed due to successful execution

**Mitigation:**
- âœ… **FIXED:** Set `plan_was_executed = True` when plan is removed from `self.plans` (during removal, not after)
- âœ… Track `plan_was_executed` flag during plan removal (atomic with removal operation)
- âœ… Only update `_plan_last_check` if plan still exists (wasn't removed due to successful execution)
- âœ… `_cleanup_plan_resources` will clean up tracking dicts anyway
- âœ… Plan removal and flag setting happen atomically within the same lock

---

## ðŸ“ **Implementation Checklist**

### **Phase 1: Configuration & Infrastructure**
- [ ] **CRITICAL:** Integrate config loading into existing `__init__` method (don't create separate loader)
- [ ] **CRITICAL:** Use deep merge for nested config dicts (preserve existing config values)
- [ ] Add `_plan_types`, `_plan_last_check`, `_plan_last_price`, `_m1_latest_candle_times` dicts to `__init__` (after line 843)
- [ ] **CRITICAL:** Initialize `prefetch_thread = None` in `__init__` for proper cleanup
- [ ] Implement `_detect_plan_type()` method
- [ ] **DO NOT** modify TradePlan dataclass - use separate tracking dicts
- [ ] Add cleanup of tracking dicts in `_cleanup_plan_resources()` method (around line 1390)
- [ ] **NOTE:** `_m1_cache_timestamps` stores float timestamps (time.time()), not datetime objects
- [ ] **NOTE:** `_m1_latest_candle_times` stores datetime objects (UTC-aware)
- [ ] **NOTE:** JSON config file should have `{"optimized_intervals": {...}}` structure
- [ ] Unit tests for plan type detection

### **Phase 2: Adaptive Intervals**
- [ ] **CRITICAL:** Preserve ALL existing monitor loop logic (lines 5804-6205)
- [ ] Implement `_calculate_adaptive_interval()` method with error handling
- [ ] **CRITICAL:** Add adaptive interval check AFTER expiration/cancellation checks (around line 6011, after cancellation check)
- [ ] **CRITICAL:** Only update `_plan_last_check` if plan was actually checked (not skipped)
- [ ] **CRITICAL:** Get plans FIRST, then fetch prices (fix order - plans must be fetched before price batch)
- [ ] Add `_get_current_prices_batch()` method with error handling (call AFTER getting plans, around line 5916)
- [ ] **OPTIMIZATION:** Only fetch prices if there are pending plans and features enabled
- [ ] Add error handling for price fetch failures (graceful degradation)
- [ ] Add error handling for Quote dataclass attribute access (fallback to symbol_info_tick)
- [ ] **DO NOT** replace existing loop - integrate new checks with early returns
- [ ] Test interval logic without breaking existing functionality
- [ ] **NOTE:** Tracking dicts accessed only from monitor thread - no locks needed

### **Phase 3: Smart Caching**
- [ ] **CRITICAL:** Use existing `_m1_data_cache` and `_m1_cache_timestamps` structures (lines 720-722)
- [ ] **CRITICAL:** Initialize `_m1_latest_candle_times` in `__init__` (not in method)
- [ ] Implement `_invalidate_cache_on_candle_close()` method using existing cache
- [ ] **CRITICAL:** Normalize symbol in `_invalidate_cache_on_candle_close()` for consistency
- [ ] **CRITICAL:** Handle all candle time formats (datetime, int, float) with proper UTC timezone handling
- [ ] Call invalidation from `_monitor_loop()` before M1 batch refresh (around line 5816)
- [ ] Update existing cache TTL logic in `self.config['m1_integration']['cache_duration_seconds']` (line 714)
- [ ] Implement `_prefetch_data_before_expiry()` method with existing cache structures
- [ ] Add error handling for pre-fetch failures (non-critical, continue with other symbols)
- [ ] Add pre-fetch thread start in `start()` method (around line 6254, after monitor thread start)
- [ ] Add pre-fetch thread cleanup in `stop()` method (around line 6311)
- [ ] Test cache invalidation with existing cache structure

### **Phase 4: Conditional Checks**
- [ ] Implement `_should_check_plan()` method
- [ ] **CRITICAL:** Add price proximity filter BEFORE existing condition checks (early return)
- [ ] Preserve all existing checks after proximity filter
- [ ] Add logging for skipped checks
- [ ] Test proximity filter effectiveness
- [ ] Ensure graceful degradation if disabled

### **Phase 5: Batch Operations**
- [ ] Verify MT5Service.get_quote() API exists and returns Quote dataclass
- [ ] Implement `_get_current_prices_batch()` method with error handling
- [ ] **CRITICAL:** Add error handling for Quote dataclass attribute access (try/except around quote.bid/ask)
- [ ] Add fallback to mt5.symbol_info_tick() if quote is None or attribute access fails
- [ ] Use plans_lock when accessing self.plans
- [ ] **Optional:** Implement batch database operations (existing `_update_plan_status()` already uses write queue)
- [ ] Test batch operation performance

### **Phase 6: Testing & Validation**
- [ ] Create unit tests
- [ ] Create integration tests
- [ ] Run performance tests
- [ ] Validate success metrics

### **Phase 7: Rollout**
- [ ] Create feature flag
- [ ] Gradual rollout plan
- [ ] Set up monitoring
- [ ] Configure alerts
- [ ] Full rollout

---

## ðŸŽ¯ **Timeline**

| Phase | Duration | Start Date | End Date |
|-------|----------|------------|----------|
| **Phase 1:** Configuration & Infrastructure | 1 day | T+0 | T+1 |
| **Phase 2:** Adaptive Intervals | 1 day | T+1 | T+2 |
| **Phase 3:** Smart Caching | 1 day | T+2 | T+3 |
| **Phase 4:** Conditional Checks | 1 day | T+3 | T+4 |
| **Phase 5:** Batch Operations | 1 day | T+4 | T+5 |
| **Phase 6:** Testing & Validation | 2 days | T+5 | T+7 |
| **Phase 7:** Rollout | 1 day | T+7 | T+8 |
| **Total** | **8 days** | | |

---

## ðŸ“š **Documentation**

### **Files to Update:**
1. `docs/AUTO_EXECUTION_SYSTEM.md` - Add adaptive intervals section
2. `docs/CACHING_STRATEGY.md` - Update caching documentation
3. `docs/PERFORMANCE_OPTIMIZATION.md` - Add optimization details
4. `config/README.md` - Document new configuration options

---

## âš ï¸ **Critical Integration Fixes Applied**

### **Issues Found and Fixed:**

1. âœ… **Config Loading:** Changed from separate JSON loader to integration with existing `self.config` dict
2. âœ… **Cache Structure:** Fixed to use existing `_m1_data_cache` and `_m1_cache_timestamps` (not non-existent `_cache`)
3. âœ… **Cache Timestamp Format:** Fixed to use float timestamps (time.time()) not datetime objects for `_m1_cache_timestamps`
4. âœ… **Candle Time Tracking:** Added separate `_m1_latest_candle_times` dict for candle-close detection
5. âœ… **TradePlan Modification:** Changed to use separate tracking dicts instead of modifying dataclass
6. âœ… **Monitor Loop:** Fixed to preserve ALL existing logic (expiration, re-evaluation, M1 refresh, etc.)
7. âœ… **Price Fetching:** Added proper error handling and fallback to mt5.symbol_info_tick()
8. âœ… **Conditional Checks Config:** Fixed to use `optimized_intervals.conditional_checks` path
9. âœ… **Database Operations:** Noted that existing `_update_plan_status()` already uses write queue
10. âœ… **Thread Management:** Added proper start/stop for pre-fetch thread
11. âœ… **Check Order Logic:** Fixed adaptive interval check to happen AFTER expiration/cancellation checks
12. âœ… **Last Check Time Update:** Fixed to only update `_plan_last_check` if plan was actually checked (not skipped)
13. âœ… **Error Handling:** Added error handling for adaptive interval calculation and price fetching
14. âœ… **Performance Optimization:** Only fetch prices if there are pending plans
15. âœ… **Thread Safety Note:** Documented that tracking dicts are accessed only from monitor thread (no locks needed)

### **Key Integration Points:**

- âœ… **Feature Flag:** All optimizations disabled by default (safe)
- âœ… **Graceful Degradation:** System works normally if config disabled
- âœ… **Backward Compatible:** Existing functionality preserved
- âœ… **Thread Safety:** Uses existing `plans_lock` where needed

---

## âœ… **Conclusion**

This implementation plan provides a comprehensive roadmap for implementing optimized 10-second intervals for M1 micro-scalp plans. The plan includes:

- âœ… **Adaptive intervals** for optimal resource usage
- âœ… **Smart caching** to reduce API calls (using existing cache structures)
- âœ… **Conditional checks** to skip unnecessary work
- âœ… **Batch operations** for efficiency
- âœ… **Comprehensive testing** to ensure quality
- âœ… **Gradual rollout** to minimize risk
- âœ… **Integration fixes** to work with existing codebase

**Expected Outcome:**
- Catch 50-70% more fast-moving M1 micro-scalp opportunities
- Maintain efficient resource usage (CPU: 8-18%)
- Reduce missed trades by 50-70%

**Next Steps:**
1. Review and approve plan
2. Begin Phase 1 implementation
3. Follow timeline and checklist
4. Monitor and validate success metrics

**âš ï¸ IMPORTANT:** All optimizations are **disabled by default**. Enable via config file after testing.
